{
 "metadata": {
  "name": "",
  "signature": "sha256:da3bbbf14600a4f886be83a70e89cf6cfc45e9a001030f15e77bc873c66514b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Building a movie recommender using Collaborative Filtering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook explains how to use the [MovieLens dataset](http://grouplens.org/datasets/movielens/) to build a movie recommender using [collaborative filtering](https://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering) with [Spark's Alternating Least Saqures](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) implementation. It is organised in two parts. The first one is about getting and parsing movies and ratings data into Spark RDDs. The second is about building the recommender and persisting it for later use in our on-line recommender system.    "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting and processing the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to build an on-line movie recommender using Spark, we need to have our model data as preprocessed as possible. Parsing the dataset and building the model everytime a new recommendation needs to be done is not the best of the strategies."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The list of task we can pre-compute includes:  \n",
      "\n",
      "- Loading and parsing the dataset. Persisting the resulting RDD for later use.  \n",
      "- Building the recommender model using the complete dataset. Persist the dataset for later use.  \n",
      "\n",
      "This notebook explains the first of these tasks.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "File download"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "GroupLens Research has collected and made available rating data sets from the [MovieLens web site](http://movielens.org). The data sets were collected over various periods of time, depending on the size of the set. They can be found [here](http://grouplens.org/datasets/movielens/).   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In our case, we will use the latest datasets:  \n",
      "\n",
      "- Small: 100,000 ratings and 2,488 tag applications applied to 8,570 movies by 706 users. Last updated 4/2015.  \n",
      "- Full: 21,000,000 ratings and 470,000 tag applications applied to 27,000 movies by 230,000 users. Last updated 4/2015.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
      "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also need to define download locations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "datasets_path = os.path.join('..', 'datasets')\n",
      "\n",
      "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
      "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can proceed with both downloads."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "\n",
      "small_f = urllib.urlretrieve (small_dataset_url, small_dataset_path)\n",
      "complete_f = urllib.urlretrieve (complete_dataset_url, complete_dataset_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Both of them are zip files containing a folder with ratings, movies, etc. We need to extract them into its individual folders so we can use each file later on.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import zipfile\n",
      "\n",
      "with zipfile.ZipFile(small_dataset_path, \"r\") as z:\n",
      "    z.extractall(datasets_path)\n",
      "\n",
      "with zipfile.ZipFile(complete_dataset_path, \"r\") as z:\n",
      "    z.extractall(datasets_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Loading and parsing datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No we are ready to read in each of the files and create an RDD consisting of parsed lines.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each line in the ratings dataset (`ratings.csv`) is formatted as:  \n",
      "\n",
      "`userId,movieId,rating,timestamp`  \n",
      "\n",
      "Each line in the movies (`movies.csv`) dataset is formatted as:  \n",
      "\n",
      "`movieId,title,genres`  \n",
      "\n",
      "Were *genres* has the format:  \n",
      "\n",
      "`Genre1|Genre2|Genre3...`\n",
      "\n",
      "The tags file (`tags.csv`) has the format:  \n",
      "\n",
      "`userId,movieId,tag,timestamp`  \n",
      "\n",
      "And finally, the `links.csv` file has the format:  \n",
      "\n",
      "`movieId,imdbId,tmdbId`  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The format of these files is uniform and simple, so we can use Python [`split()`](https://docs.python.org/2/library/stdtypes.html#str.split) to parse their lines once they are loaded into RDDs. Parsing the movies and ratings files yields two RDDs:  \n",
      "\n",
      "* For each line in the ratings dataset, we create a tuple of `(UserID, MovieID, Rating)`. We drop the *timestamp* because we do not need it for this recommender.  \n",
      "* For each line in the movies dataset, we create a tuple of `(MovieID, Title)`. We drop the *genres* because we do not use them for this recommender.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So let's load the raw ratings data. We need to filter out the header, included in each file.    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
      "\n",
      "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
      "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can parse the raw data into a new RDD.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
      "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For illustrative purposes, we can take the first few lines of our RDD to see the result. In the final script we don't call any Spark action (e.g. `take`) until needed, since they trigger actual computations in the cluster.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_ratings_data.take(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(u'1', u'6', u'2.0'), (u'1', u'22', u'3.0'), (u'1', u'32', u'2.0')]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We proceed in a similar way with the `movies.csv` file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
      "\n",
      "small_movies_raw_data = sc.textFile(small_movies_file)\n",
      "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
      "\n",
      "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
      "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
      "    \n",
      "small_movies_data.take(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[(u'1', u'Toy Story (1995)'),\n",
        " (u'2', u'Jumanji (1995)'),\n",
        " (u'3', u'Grumpier Old Men (1995)')]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a recommender using Alternating Least Squares"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using the complete dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Persisting the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to persist the base model for later use in our on-line recommendations.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}